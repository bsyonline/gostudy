package main

import (
	"encoding/json"
	"fmt"
)

func main() {
	jsonStr := `{"alerts":[{"annotations":{"description":"KubeControllerManager has disappeared from Prometheus target discovery.","runbook_url":"https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubecontrollermanagerdown","summary":"Target disappeared from Prometheus target discovery."},"endsAt":"0001-01-01T00:00:00Z","fingerprint":"838121cc4ca56ab5","generatorURL":"http://prometheus-k8s-1:9090/graph?g0.expr=absent%28up%7Bjob%3D%22kube-controller-manager%22%7D+%3D%3D+1%29\u0026g0.tab=1","labels":{"alertname":"KubeControllerManagerDown","prometheus":"monitoring/k8s","severity":"critical"},"startsAt":"2023-11-28T10:01:26.887Z","status":"firing"},{"annotations":{"description":"KubeScheduler has disappeared from Prometheus target discovery.","runbook_url":"https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeschedulerdown","summary":"Target disappeared from Prometheus target discovery."},"endsAt":"0001-01-01T00:00:00Z","fingerprint":"92b2f1b7ee31decf","generatorURL":"http://prometheus-k8s-1:9090/graph?g0.expr=absent%28up%7Bjob%3D%22kube-scheduler%22%7D+%3D%3D+1%29\u0026g0.tab=1","labels":{"alertname":"KubeSchedulerDown","prometheus":"monitoring/k8s","severity":"critical"},"startsAt":"2023-11-28T10:01:29.001Z","status":"firing"},{"annotations":{"description":"test cpu cores","summary":"test cpu cores"},"endsAt":"0001-01-01T00:00:00Z","fingerprint":"6ed802906883f9e2","generatorURL":"http://prometheus-k8s-0:9090/graph?g0.expr=count%28count+by+%28cpu%29+%28node_cpu_seconds_total%7Binstance%3D~%22k8s-worker1.sdns.dev.cloud%22%2Cmode%3D%22system%22%7D%29%29+%3E+0\u0026g0.tab=1","labels":{"alertname":"TestCpuCore","prometheus":"monitoring/k8s","severity":"critical"},"startsAt":"2023-11-28T07:02:30.84Z","status":"firing"},{"annotations":{"description":"This is an alert meant to ensure that the entire alerting pipeline is functional.\nThis alert is always firing, therefore it should always be firing in Alertmanager\nand always fire against a receiver. There are integrations with various notification\nmechanisms that send a notification when this alert is not firing. For example the\n\"DeadMansSnitch\" integration in PagerDuty.\n","runbook_url":"https://runbooks.prometheus-operator.dev/runbooks/general/watchdog","summary":"An alert that should always be firing to certify that Alertmanager is working properly."},"endsAt":"0001-01-01T00:00:00Z","fingerprint":"e1749c6acab64267","generatorURL":"http://prometheus-k8s-1:9090/graph?g0.expr=vector%281%29\u0026g0.tab=1","labels":{"alertname":"Watchdog","prometheus":"monitoring/k8s","severity":"none"},"startsAt":"2023-11-28T07:01:04.637Z","status":"firing"},{"annotations":{"description":"test gpu cores","summary":"test gpu cores"},"endsAt":"0001-01-01T00:00:00Z","fingerprint":"77795e7b5b039f23","generatorURL":"http://prometheus-k8s-1:9090/graph?g0.expr=sum+by+%28node_name%29+%28avg+by+%28node_name%29+%28rate%28DCGM_FI_DEV_GPU_UTIL%5B2m%5D%29%29%29+%3D%3D+0\u0026g0.tab=1","labels":{"alertname":"TestGpuCore","node_name":"k8s-worker-4090-1.sdns.dev.cloud","prometheus":"monitoring/k8s","severity":"critical"},"startsAt":"2023-12-05T08:21:49.049Z","status":"firing"},{"annotations":{"description":"test gpu cores","summary":"test gpu cores"},"endsAt":"0001-01-01T00:00:00Z","fingerprint":"0aafff281fa1f336","generatorURL":"http://prometheus-k8s-1:9090/graph?g0.expr=sum+by+%28node_name%29+%28avg+by+%28node_name%29+%28rate%28DCGM_FI_DEV_GPU_UTIL%5B2m%5D%29%29%29+%3D%3D+0\u0026g0.tab=1","labels":{"alertname":"TestGpuCore","node_name":"k8s-worker-4090-2.sdns.dev.cloud","prometheus":"monitoring/k8s","severity":"critical"},"startsAt":"2023-12-05T08:21:49.049Z","status":"firing"},{"annotations":{"description":"test gpu cores","summary":"test gpu cores"},"endsAt":"0001-01-01T00:00:00Z","fingerprint":"d631c2040fbecb65","generatorURL":"http://prometheus-k8s-1:9090/graph?g0.expr=sum+by+%28node_name%29+%28avg+by+%28node_name%29+%28rate%28DCGM_FI_DEV_GPU_UTIL%5B2m%5D%29%29%29+%3D%3D+0\u0026g0.tab=1","labels":{"alertname":"TestGpuCore","node_name":"k8s-worker-4090-3.sdns.dev.cloud","prometheus":"monitoring/k8s","severity":"critical"},"startsAt":"2023-12-05T08:21:49.049Z","status":"firing"},{"annotations":{"description":"test gpu cores","summary":"test gpu cores"},"endsAt":"0001-01-01T00:00:00Z","fingerprint":"19d915066e2158b8","generatorURL":"http://prometheus-k8s-1:9090/graph?g0.expr=sum+by+%28node_name%29+%28avg+by+%28node_name%29+%28rate%28DCGM_FI_DEV_GPU_UTIL%5B2m%5D%29%29%29+%3D%3D+0\u0026g0.tab=1","labels":{"alertname":"TestGpuCore","node_name":"k8s-worker-4090-4.sdns.dev.cloud","prometheus":"monitoring/k8s","severity":"critical"},"startsAt":"2023-12-05T08:21:49.049Z","status":"firing"},{"annotations":{"description":"test gpu cores","summary":"test gpu cores"},"endsAt":"0001-01-01T00:00:00Z","fingerprint":"4618b6b3c6f632a7","generatorURL":"http://prometheus-k8s-1:9090/graph?g0.expr=sum+by+%28node_name%29+%28avg+by+%28node_name%29+%28rate%28DCGM_FI_DEV_GPU_UTIL%5B2m%5D%29%29%29+%3D%3D+0\u0026g0.tab=1","labels":{"alertname":"TestGpuCore","node_name":"k8s-worker-4090-5.sdns.dev.cloud","prometheus":"monitoring/k8s","severity":"critical"},"startsAt":"2023-12-05T08:21:49.049Z","status":"firing"},{"annotations":{"description":"test gpu cores","summary":"test gpu cores"},"endsAt":"0001-01-01T00:00:00Z","fingerprint":"5fcbff2eb5cd83ea","generatorURL":"http://prometheus-k8s-1:9090/graph?g0.expr=sum+by+%28node_name%29+%28avg+by+%28node_name%29+%28rate%28DCGM_FI_DEV_GPU_UTIL%5B2m%5D%29%29%29+%3D%3D+0\u0026g0.tab=1","labels":{"alertname":"TestGpuCore","node_name":"k8s-worker-4090-6.sdns.dev.cloud","prometheus":"monitoring/k8s","severity":"critical"},"startsAt":"2023-12-05T08:21:49.049Z","status":"firing"},{"annotations":{"description":"test gpu cores","summary":"test gpu cores"},"endsAt":"0001-01-01T00:00:00Z","fingerprint":"5beb49fd48263b29","generatorURL":"http://prometheus-k8s-1:9090/graph?g0.expr=sum+by+%28node_name%29+%28avg+by+%28node_name%29+%28rate%28DCGM_FI_DEV_GPU_UTIL%5B2m%5D%29%29%29+%3D%3D+0\u0026g0.tab=1","labels":{"alertname":"TestGpuCore","node_name":"k8s-worker-4090-7.sdns.dev.cloud","prometheus":"monitoring/k8s","severity":"critical"},"startsAt":"2023-12-05T08:21:49.049Z","status":"firing"},{"annotations":{"description":"test gpu cores","summary":"test gpu cores"},"endsAt":"0001-01-01T00:00:00Z","fingerprint":"3e8efcb051603afc","generatorURL":"http://prometheus-k8s-1:9090/graph?g0.expr=sum+by+%28node_name%29+%28avg+by+%28node_name%29+%28rate%28DCGM_FI_DEV_GPU_UTIL%5B2m%5D%29%29%29+%3D%3D+0\u0026g0.tab=1","labels":{"alertname":"TestGpuCore","node_name":"k8s-worker-4090-8.sdns.dev.cloud","prometheus":"monitoring/k8s","severity":"critical"},"startsAt":"2023-12-05T08:21:49.049Z","status":"firing"},{"annotations":{"description":"test gpu cores","summary":"test gpu cores"},"endsAt":"0001-01-01T00:00:00Z","fingerprint":"54266ba405165368","generatorURL":"http://prometheus-k8s-1:9090/graph?g0.expr=sum+by+%28node_name%29+%28avg+by+%28node_name%29+%28rate%28DCGM_FI_DEV_GPU_UTIL%5B2m%5D%29%29%29+%3D%3D+0\u0026g0.tab=1","labels":{"alertname":"TestGpuCore","node_name":"k8s-worker1.sdns.dev.cloud","prometheus":"monitoring/k8s","severity":"critical"},"startsAt":"2023-12-05T08:21:49.049Z","status":"firing"},{"annotations":{"description":"test gpu cores","summary":"test gpu cores"},"endsAt":"0001-01-01T00:00:00Z","fingerprint":"e61e2a4d383945d9","generatorURL":"http://prometheus-k8s-1:9090/graph?g0.expr=sum+by+%28node_name%29+%28avg+by+%28node_name%29+%28rate%28DCGM_FI_DEV_GPU_UTIL%5B2m%5D%29%29%29+%3D%3D+0\u0026g0.tab=1","labels":{"alertname":"TestGpuCore","node_name":"k8s-worker2.sdns.dev.cloud","prometheus":"monitoring/k8s","severity":"critical"},"startsAt":"2023-12-05T08:21:49.049Z","status":"firing"},{"annotations":{"description":"test gpu cores","summary":"test gpu cores"},"endsAt":"0001-01-01T00:00:00Z","fingerprint":"28ddc6e9d4df875a","generatorURL":"http://prometheus-k8s-1:9090/graph?g0.expr=sum+by+%28node_name%29+%28avg+by+%28node_name%29+%28rate%28DCGM_FI_DEV_GPU_UTIL%5B2m%5D%29%29%29+%3D%3D+0\u0026g0.tab=1","labels":{"alertname":"TestGpuCore","node_name":"k8s-worker3.sdns.dev.cloud","prometheus":"monitoring/k8s","severity":"critical"},"startsAt":"2023-12-05T08:21:49.049Z","status":"firing"},{"annotations":{"description":"test gpu cores","summary":"test gpu cores"},"endsAt":"0001-01-01T00:00:00Z","fingerprint":"5584987136116dd3","generatorURL":"http://prometheus-k8s-1:9090/graph?g0.expr=sum+by+%28node_name%29+%28avg+by+%28node_name%29+%28rate%28DCGM_FI_DEV_GPU_UTIL%5B2m%5D%29%29%29+%3D%3D+0\u0026g0.tab=1","labels":{"alertname":"TestGpuCore","node_name":"k8s-worker4.sdns.dev.cloud","prometheus":"monitoring/k8s","severity":"critical"},"startsAt":"2023-12-05T08:21:49.049Z","status":"firing"},{"annotations":{"description":"Job default/prometheus-project-kube-pr-admission-create is taking more than 12h 0m 0s to complete.","runbook_url":"https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubejobnotcompleted","summary":"Job did not complete in time"},"endsAt":"0001-01-01T00:00:00Z","fingerprint":"a17f229b73006275","generatorURL":"http://prometheus-k8s-1:9090/graph?g0.expr=time%28%29+-+max+by+%28namespace%2C+job_name%2C+cluster%29+%28kube_job_status_start_time%7Bjob%3D%22kube-state-metrics%22%7D+and+kube_job_status_active%7Bjob%3D%22kube-state-metrics%22%7D+%3E+0%29+%3E+43200\u0026g0.tab=1","labels":{"alertname":"KubeJobNotCompleted","job_name":"prometheus-project-kube-pr-admission-create","namespace":"default","prometheus":"monitoring/k8s","severity":"warning"},"startsAt":"2023-11-28T07:01:32.991Z","status":"firing"},{"annotations":{"description":"Pod knative-serving/net-istio-webhook-7ff9fdf999-cb8mf has been in a non-ready state for longer than 15 minutes.","runbook_url":"https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepodnotready","summary":"Pod has been in a non-ready state for more than 15 minutes."},"endsAt":"0001-01-01T00:00:00Z","fingerprint":"4784baf5980415ae","generatorURL":"http://prometheus-k8s-0:9090/graph?g0.expr=sum+by+%28namespace%2C+pod%2C+cluster%29+%28max+by+%28namespace%2C+pod%2C+cluster%29+%28kube_pod_status_phase%7Bjob%3D%22kube-state-metrics%22%2Cphase%3D~%22Pending%7CUnknown%7CFailed%22%7D%29+%2A+on+%28namespace%2C+pod%2C+cluster%29+group_left+%28owner_kind%29+topk+by+%28namespace%2C+pod%2C+cluster%29+%281%2C+max+by+%28namespace%2C+pod%2C+owner_kind%2C+cluster%29+%28kube_pod_owner%7Bowner_kind%21%3D%22Job%22%7D%29%29%29+%3E+0\u0026g0.tab=1","labels":{"alertname":"KubePodNotReady","namespace":"knative-serving","pod":"net-istio-webhook-7ff9fdf999-cb8mf","prometheus":"monitoring/k8s","severity":"warning"},"startsAt":"2023-11-28T10:02:02.991Z","status":"firing"},{"annotations":{"description":"61.86% throttling of CPU in namespace monitoring for container blackbox-exporter in pod blackbox-exporter-6fd586b445-crwmz.","runbook_url":"https://runbooks.prometheus-operator.dev/runbooks/kubernetes/cputhrottlinghigh","summary":"Processes experience elevated CPU throttling."},"endsAt":"0001-01-01T00:00:00Z","fingerprint":"e957701f619084a9","generatorURL":"http://prometheus-k8s-0:9090/graph?g0.expr=sum+by+%28container%2C+pod%2C+namespace%29+%28increase%28container_cpu_cfs_throttled_periods_total%7Bcontainer%21%3D%22%22%7D%5B5m%5D%29%29+%2F+sum+by+%28container%2C+pod%2C+namespace%29+%28increase%28container_cpu_cfs_periods_total%5B5m%5D%29%29+%3E+%2825+%2F+100%29\u0026g0.tab=1","labels":{"alertname":"CPUThrottlingHigh","container":"blackbox-exporter","namespace":"monitoring","pod":"blackbox-exporter-6fd586b445-crwmz","prometheus":"monitoring/k8s","severity":"info"},"startsAt":"2023-11-28T10:02:08.308Z","status":"firing"},{"annotations":{"description":"30.5% throttling of CPU in namespace monitoring for container grafana in pod grafana-6849bbf859-c5wkt.","runbook_url":"https://runbooks.prometheus-operator.dev/runbooks/kubernetes/cputhrottlinghigh","summary":"Processes experience elevated CPU throttling."},"endsAt":"0001-01-01T00:00:00Z","fingerprint":"4268a0c03e205843","generatorURL":"http://prometheus-k8s-0:9090/graph?g0.expr=sum+by+%28container%2C+pod%2C+namespace%29+%28increase%28container_cpu_cfs_throttled_periods_total%7Bcontainer%21%3D%22%22%7D%5B5m%5D%29%29+%2F+sum+by+%28container%2C+pod%2C+namespace%29+%28increase%28container_cpu_cfs_periods_total%5B5m%5D%29%29+%3E+%2825+%2F+100%29\u0026g0.tab=1","labels":{"alertname":"CPUThrottlingHigh","container":"grafana","namespace":"monitoring","pod":"grafana-6849bbf859-c5wkt","prometheus":"monitoring/k8s","severity":"info"},"startsAt":"2023-12-01T09:47:08.308Z","status":"firing"},{"annotations":{"description":"73.82% throttling of CPU in namespace monitoring for container kube-rbac-proxy-main in pod kube-state-metrics-5f7fff46f7-z242x.","runbook_url":"https://runbooks.prometheus-operator.dev/runbooks/kubernetes/cputhrottlinghigh","summary":"Processes experience elevated CPU throttling."},"endsAt":"0001-01-01T00:00:00Z","fingerprint":"885420b6e105c581","generatorURL":"http://prometheus-k8s-0:9090/graph?g0.expr=sum+by+%28container%2C+pod%2C+namespace%29+%28increase%28container_cpu_cfs_throttled_periods_total%7Bcontainer%21%3D%22%22%7D%5B5m%5D%29%29+%2F+sum+by+%28container%2C+pod%2C+namespace%29+%28increase%28container_cpu_cfs_periods_total%5B5m%5D%29%29+%3E+%2825+%2F+100%29\u0026g0.tab=1","labels":{"alertname":"CPUThrottlingHigh","container":"kube-rbac-proxy-main","namespace":"monitoring","pod":"kube-state-metrics-5f7fff46f7-z242x","prometheus":"monitoring/k8s","severity":"info"},"startsAt":"2023-11-28T10:02:08.308Z","status":"firing"},{"annotations":{"description":"68.24% throttling of CPU in namespace monitoring for container node-exporter in pod node-exporter-84zkg.","runbook_url":"https://runbooks.prometheus-operator.dev/runbooks/kubernetes/cputhrottlinghigh","summary":"Processes experience elevated CPU throttling."},"endsAt":"0001-01-01T00:00:00Z","fingerprint":"542a5d16ffc734f5","generatorURL":"http://prometheus-k8s-0:9090/graph?g0.expr=sum+by+%28container%2C+pod%2C+namespace%29+%28increase%28container_cpu_cfs_throttled_periods_total%7Bcontainer%21%3D%22%22%7D%5B5m%5D%29%29+%2F+sum+by+%28container%2C+pod%2C+namespace%29+%28increase%28container_cpu_cfs_periods_total%5B5m%5D%29%29+%3E+%2825+%2F+100%29\u0026g0.tab=1","labels":{"alertname":"CPUThrottlingHigh","container":"node-exporter","namespace":"monitoring","pod":"node-exporter-84zkg","prometheus":"monitoring/k8s","severity":"info"},"startsAt":"2023-11-28T10:02:08.308Z","status":"firing"},{"annotations":{"description":"69.36% throttling of CPU in namespace monitoring for container node-exporter in pod node-exporter-9psjd.","runbook_url":"https://runbooks.prometheus-operator.dev/runbooks/kubernetes/cputhrottlinghigh","summary":"Processes experience elevated CPU throttling."},"endsAt":"0001-01-01T00:00:00Z","fingerprint":"2c2ebd465f771059","generatorURL":"http://prometheus-k8s-0:9090/graph?g0.expr=sum+by+%28container%2C+pod%2C+namespace%29+%28increase%28container_cpu_cfs_throttled_periods_total%7Bcontainer%21%3D%22%22%7D%5B5m%5D%29%29+%2F+sum+by+%28container%2C+pod%2C+namespace%29+%28increase%28container_cpu_cfs_periods_total%5B5m%5D%29%29+%3E+%2825+%2F+100%29\u0026g0.tab=1","labels":{"alertname":"CPUThrottlingHigh","container":"node-exporter","namespace":"monitoring","pod":"node-exporter-9psjd","prometheus":"monitoring/k8s","severity":"info"},"startsAt":"2023-11-28T10:02:08.308Z","status":"firing"},{"annotations":{"description":"72.24% throttling of CPU in namespace monitoring for container node-exporter in pod node-exporter-9thzc.","runbook_url":"https://runbooks.prometheus-operator.dev/runbooks/kubernetes/cputhrottlinghigh","summary":"Processes experience elevated CPU throttling."},"endsAt":"0001-01-01T00:00:00Z","fingerprint":"d6b9d7d9a8a080a5","generatorURL":"http://prometheus-k8s-0:9090/graph?g0.expr=sum+by+%28container%2C+pod%2C+namespace%29+%28increase%28container_cpu_cfs_throttled_periods_total%7Bcontainer%21%3D%22%22%7D%5B5m%5D%29%29+%2F+sum+by+%28container%2C+pod%2C+namespace%29+%28increase%28container_cpu_cfs_periods_total%5B5m%5D%29%29+%3E+%2825+%2F+100%29\u0026g0.tab=1","labels":{"alertname":"CPUThrottlingHigh","container":"node-exporter","namespace":"monitoring","pod":"node-exporter-9thzc","prometheus":"monitoring/k8s","severity":"info"},"startsAt":"2023-11-28T10:02:08.308Z","status":"firing"},{"annotations":{"description":"73.28% throttling of CPU in namespace monitoring for container node-exporter in pod node-exporter-bb6rs.","runbook_url":"https://runbooks.prometheus-operator.dev/runbooks/kubernetes/cputhrottlinghigh","summary":"Processes experience elevated CPU throttling."},"endsAt":"0001-01-01T00:00:00Z","fingerprint":"5c45c601266b82ee","generatorURL":"http://prometheus-k8s-0:9090/graph?g0.expr=sum+by+%28container%2C+pod%2C+namespace%29+%28increase%28container_cpu_cfs_throttled_periods_total%7Bcontainer%21%3D%22%22%7D%5B5m%5D%29%29+%2F+sum+by+%28container%2C+pod%2C+namespace%29+%28increase%28container_cpu_cfs_periods_total%5B5m%5D%29%29+%3E+%2825+%2F+100%29\u0026g0.tab=1","labels":{"alertname":"CPUThrottlingHigh","container":"node-exporter","namespace":"monitoring","pod":"node-exporter-bb6rs","prometheus":"monitoring/k8s","severity":"info"},"startsAt":"2023-11-28T10:02:08.308Z","status":"firing"},{"annotations":{"description":"70.1% throttling of CPU in namespace monitoring for container node-exporter in pod node-exporter-bftvl.","runbook_url":"https://runbooks.prometheus-operator.dev/runbooks/kubernetes/cputhrottlinghigh","summary":"Processes experience elevated CPU throttling."},"endsAt":"0001-01-01T00:00:00Z","fingerprint":"65f61cea4cd89bdf","generatorURL":"http://prometheus-k8s-0:9090/graph?g0.expr=sum+by+%28container%2C+pod%2C+namespace%29+%28increase%28container_cpu_cfs_throttled_periods_total%7Bcontainer%21%3D%22%22%7D%5B5m%5D%29%29+%2F+sum+by+%28container%2C+pod%2C+namespace%29+%28increase%28container_cpu_cfs_periods_total%5B5m%5D%29%29+%3E+%2825+%2F+100%29\u0026g0.tab=1","labels":{"alertname":"CPUThrottlingHigh","container":"node-exporter","namespace":"monitoring","pod":"node-exporter-bftvl","prometheus":"monitoring/k8s","severity":"info"},"startsAt":"2023-11-28T10:02:08.308Z","status":"firing"},{"annotations":{"description":"71.33% throttling of CPU in namespace monitoring for container node-exporter in pod node-exporter-cczr9.","runbook_url":"https://runbooks.prometheus-operator.dev/runbooks/kubernetes/cputhrottlinghigh","summary":"Processes experience elevated CPU throttling."},"endsAt":"0001-01-01T00:00:00Z","fingerprint":"1121b94100ec5cb2","generatorURL":"http://prometheus-k8s-1:9090/graph?g0.expr=sum+by+%28container%2C+pod%2C+namespace%29+%28increase%28container_cpu_cfs_throttled_periods_total%7Bcontainer%21%3D%22%22%7D%5B5m%5D%29%29+%2F+sum+by+%28container%2C+pod%2C+namespace%29+%28increase%28container_cpu_cfs_periods_total%5B5m%5D%29%29+%3E+%2825+%2F+100%29\u0026g0.tab=1","labels":{"alertname":"CPUThrottlingHigh","container":"node-exporter","namespace":"monitoring","pod":"node-exporter-cczr9","prometheus":"monitoring/k8s","severity":"info"},"startsAt":"2023-11-28T10:02:38.308Z","status":"firing"},{"annotations":{"description":"79.35% throttling of CPU in namespace monitoring for container node-exporter in pod node-exporter-d4vqn.","runbook_url":"https://runbooks.prometheus-operator.dev/runbooks/kubernetes/cputhrottlinghigh","summary":"Processes experience elevated CPU throttling."},"endsAt":"0001-01-01T00:00:00Z","fingerprint":"a5bc059964734ffe","generatorURL":"http://prometheus-k8s-0:9090/graph?g0.expr=sum+by+%28container%2C+pod%2C+namespace%29+%28increase%28container_cpu_cfs_throttled_periods_total%7Bcontainer%21%3D%22%22%7D%5B5m%5D%29%29+%2F+sum+by+%28container%2C+pod%2C+namespace%29+%28increase%28container_cpu_cfs_periods_total%5B5m%5D%29%29+%3E+%2825+%2F+100%29\u0026g0.tab=1","labels":{"alertname":"CPUThrottlingHigh","container":"node-exporter","namespace":"monitoring","pod":"node-exporter-d4vqn","prometheus":"monitoring/k8s","severity":"info"},"startsAt":"2023-11-28T10:02:08.308Z","status":"firing"},{"annotations":{"description":"72.88% throttling of CPU in namespace monitoring for container node-exporter in pod node-exporter-hh8hn.","runbook_url":"https://runbooks.prometheus-operator.dev/runbooks/kubernetes/cputhrottlinghigh","summary":"Processes experience elevated CPU throttling."},"endsAt":"0001-01-01T00:00:00Z","fingerprint":"ea20ace47b2ded23","generatorURL":"http://prometheus-k8s-0:9090/graph?g0.expr=sum+by+%28container%2C+pod%2C+namespace%29+%28increase%28container_cpu_cfs_throttled_periods_total%7Bcontainer%21%3D%22%22%7D%5B5m%5D%29%29+%2F+sum+by+%28container%2C+pod%2C+namespace%29+%28increase%28container_cpu_cfs_periods_total%5B5m%5D%29%29+%3E+%2825+%2F+100%29\u0026g0.tab=1","labels":{"alertname":"CPUThrottlingHigh","container":"node-exporter","namespace":"monitoring","pod":"node-exporter-hh8hn","prometheus":"monitoring/k8s","severity":"info"},"startsAt":"2023-11-28T10:02:08.308Z","status":"firing"},{"annotations":{"description":"69.18% throttling of CPU in namespace monitoring for container node-exporter in pod node-exporter-k5kdw.","runbook_url":"https://runbooks.prometheus-operator.dev/runbooks/kubernetes/cputhrottlinghigh","summary":"Processes experience elevated CPU throttling."},"endsAt":"0001-01-01T00:00:00Z","fingerprint":"837c6f67090ed21d","generatorURL":"http://prometheus-k8s-0:9090/graph?g0.expr=sum+by+%28container%2C+pod%2C+namespace%29+%28increase%28container_cpu_cfs_throttled_periods_total%7Bcontainer%21%3D%22%22%7D%5B5m%5D%29%29+%2F+sum+by+%28container%2C+pod%2C+namespace%29+%28increase%28container_cpu_cfs_periods_total%5B5m%5D%29%29+%3E+%2825+%2F+100%29\u0026g0.tab=1","labels":{"alertname":"CPUThrottlingHigh","container":"node-exporter","namespace":"monitoring","pod":"node-exporter-k5kdw","prometheus":"monitoring/k8s","severity":"info"},"startsAt":"2023-11-28T10:02:08.308Z","status":"firing"},{"annotations":{"description":"68.66% throttling of CPU in namespace monitoring for container node-exporter in pod node-exporter-pckfk.","runbook_url":"https://runbooks.prometheus-operator.dev/runbooks/kubernetes/cputhrottlinghigh","summary":"Processes experience elevated CPU throttling."},"endsAt":"0001-01-01T00:00:00Z","fingerprint":"a6eb7d6172f36242","generatorURL":"http://prometheus-k8s-0:9090/graph?g0.expr=sum+by+%28container%2C+pod%2C+namespace%29+%28increase%28container_cpu_cfs_throttled_periods_total%7Bcontainer%21%3D%22%22%7D%5B5m%5D%29%29+%2F+sum+by+%28container%2C+pod%2C+namespace%29+%28increase%28container_cpu_cfs_periods_total%5B5m%5D%29%29+%3E+%2825+%2F+100%29\u0026g0.tab=1","labels":{"alertname":"CPUThrottlingHigh","container":"node-exporter","namespace":"monitoring","pod":"node-exporter-pckfk","prometheus":"monitoring/k8s","severity":"info"},"startsAt":"2023-11-28T10:02:08.308Z","status":"firing"},{"annotations":{"description":"71.43% throttling of CPU in namespace monitoring for container node-exporter in pod node-exporter-rm82h.","runbook_url":"https://runbooks.prometheus-operator.dev/runbooks/kubernetes/cputhrottlinghigh","summary":"Processes experience elevated CPU throttling."},"endsAt":"0001-01-01T00:00:00Z","fingerprint":"ab3c5084502057ea","generatorURL":"http://prometheus-k8s-0:9090/graph?g0.expr=sum+by+%28container%2C+pod%2C+namespace%29+%28increase%28container_cpu_cfs_throttled_periods_total%7Bcontainer%21%3D%22%22%7D%5B5m%5D%29%29+%2F+sum+by+%28container%2C+pod%2C+namespace%29+%28increase%28container_cpu_cfs_periods_total%5B5m%5D%29%29+%3E+%2825+%2F+100%29\u0026g0.tab=1","labels":{"alertname":"CPUThrottlingHigh","container":"node-exporter","namespace":"monitoring","pod":"node-exporter-rm82h","prometheus":"monitoring/k8s","severity":"info"},"startsAt":"2023-11-28T10:02:08.308Z","status":"firing"},{"annotations":{"description":"56.93% throttling of CPU in namespace monitoring for container node-exporter in pod node-exporter-rnljj.","runbook_url":"https://runbooks.prometheus-operator.dev/runbooks/kubernetes/cputhrottlinghigh","summary":"Processes experience elevated CPU throttling."},"endsAt":"0001-01-01T00:00:00Z","fingerprint":"b28aad010cac7fa9","generatorURL":"http://prometheus-k8s-0:9090/graph?g0.expr=sum+by+%28container%2C+pod%2C+namespace%29+%28increase%28container_cpu_cfs_throttled_periods_total%7Bcontainer%21%3D%22%22%7D%5B5m%5D%29%29+%2F+sum+by+%28container%2C+pod%2C+namespace%29+%28increase%28container_cpu_cfs_periods_total%5B5m%5D%29%29+%3E+%2825+%2F+100%29\u0026g0.tab=1","labels":{"alertname":"CPUThrottlingHigh","container":"node-exporter","namespace":"monitoring","pod":"node-exporter-rnljj","prometheus":"monitoring/k8s","severity":"info"},"startsAt":"2023-11-28T10:02:08.308Z","status":"firing"},{"annotations":{"description":"pod/prometheus-project-kube-pr-admission-create-2kcw8 in namespace default on container create has been in waiting state for longer than 1 hour.","runbook_url":"https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubecontainerwaiting","summary":"Pod container waiting longer than 1 hour"},"endsAt":"0001-01-01T00:00:00Z","fingerprint":"8e75a7d95eba02c5","generatorURL":"http://prometheus-k8s-0:9090/graph?g0.expr=sum+by+%28namespace%2C+pod%2C+container%2C+cluster%29+%28kube_pod_container_status_waiting_reason%7Bjob%3D%22kube-state-metrics%22%7D%29+%3E+0\u0026g0.tab=1","labels":{"alertname":"KubeContainerWaiting","container":"create","namespace":"default","pod":"prometheus-project-kube-pr-admission-create-2kcw8","prometheus":"monitoring/k8s","severity":"warning"},"startsAt":"2023-11-28T10:47:02.991Z","status":"firing"},{"annotations":{"description":"pod/net-istio-webhook-7ff9fdf999-cb8mf in namespace knative-serving on container webhook has been in waiting state for longer than 1 hour.","runbook_url":"https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubecontainerwaiting","summary":"Pod container waiting longer than 1 hour"},"endsAt":"0001-01-01T00:00:00Z","fingerprint":"1c822b6fc1cf349b","generatorURL":"http://prometheus-k8s-0:9090/graph?g0.expr=sum+by+%28namespace%2C+pod%2C+container%2C+cluster%29+%28kube_pod_container_status_waiting_reason%7Bjob%3D%22kube-state-metrics%22%7D%29+%3E+0\u0026g0.tab=1","labels":{"alertname":"KubeContainerWaiting","container":"webhook","namespace":"knative-serving","pod":"net-istio-webhook-7ff9fdf999-cb8mf","prometheus":"monitoring/k8s","severity":"warning"},"startsAt":"2023-11-28T10:47:02.991Z","status":"firing"}],"commonAnnotations":{},"commonLabels":{"prometheus":"monitoring/k8s"},"externalURL":"http://alertmanager-main-2:9093","groupKey":"{}:{}","groupLabels":{},"receiver":"webhook","status":"firing","truncatedAlerts":0,"version":"4"}`
	var alert Alert
	json.Unmarshal([]byte(jsonStr), &alert)
	alertMap := make(map[string][]string)
	for _, v := range alert.Alerts {
		alertName := v.Labels.Alertname
		severity := v.Labels.Severity
		nodeName := v.Labels.NodeName
		if nodeName != "" && severity == "critical" {
			fmt.Printf("alertName: %v\n", alertName)
			fmt.Printf("severity: %v\n", severity)
			fmt.Printf("nodeName: %v\n", nodeName)
			startAt := v.StartsAt
			fmt.Printf("startAt: %v\n", startAt)
			fmt.Println("---")
			value, has := alertMap[nodeName]
			if has {
				alertMap[nodeName] = append(value, alertName)
			} else {
				var arr []string
				arr = append(arr, alertName)
				alertMap[nodeName] = arr
			}
		}
	}
	fmt.Printf("alertMap: %v\n", alertMap)
}

type Alert struct {
	Alerts []struct {
		annotations struct {
			Description string `json:"description"`
			Summary     string `json:"summary"`
		}
		Labels struct {
			Alertname string `json:"alertname"`
			Severity  string `json:"severity"`
			NodeName  string `json:"node_name"`
		} `json:"labels"`
		StartsAt string `json:"startsAt"`
	} `json:"alerts"`
}
